{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e0776d-00fe-4a89-a71b-2348bfe42f2f",
   "metadata": {
    "editable": true,
    "id": "86e0776d-00fe-4a89-a71b-2348bfe42f2f",
    "tags": []
   },
   "source": [
    "# Checkpoint 2: Mandatory Check-In\n",
    "\n",
    "- Research Questions (1.5%).\n",
    "- Feature Engineering (2%).\n",
    "- Modeling Approaches (3%).\n",
    "- Preliminary Results (1%).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef5ee1-56cd-485d-a3a0-8f768c11a934",
   "metadata": {
    "id": "d1ef5ee1-56cd-485d-a3a0-8f768c11a934"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f809070-6be8-4922-9766-f7d6838f0930",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33318,
     "status": "ok",
     "timestamp": 1712183654879,
     "user": {
      "displayName": "Brent Allen Thorne",
      "userId": "03585588261553538781"
     },
     "user_tz": 420
    },
    "id": "5f809070-6be8-4922-9766-f7d6838f0930",
    "outputId": "112b31df-2915-483f-c45a-cccc711033c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 10:26:56.300069: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import re\n",
    "import io\n",
    "import os\n",
    "import pickle\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "COLAB = False\n",
    "\n",
    "# mount Colab drive and set library path\n",
    "if COLAB:\n",
    "  import sys\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  path = '/content/drive/My Drive/Colab Notebooks/grad_project'\n",
    "  data_dir = f'{path}/Data'\n",
    "  sys.path.insert(0, f'{path}/Source') # for colab to see local libraries\n",
    "else:\n",
    "  data_dir = f'../Data'\n",
    "\n",
    "\n",
    "from data_utils import read_files_from_zip, data2pd, show_balance, load_model_data, save_model_data\n",
    "from feature_utils import crop_and_fill\n",
    "from eda_utils import show_image\n",
    "\n",
    "\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Rescaling\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from random import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7a8712-beb4-46cc-9081-b054f7cd97de",
   "metadata": {
    "id": "ec7a8712-beb4-46cc-9081-b054f7cd97de"
   },
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf23837a-4488-491a-af93-e31f95d3af11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32816,
     "status": "ok",
     "timestamp": 1712183687683,
     "user": {
      "displayName": "Brent Allen Thorne",
      "userId": "03585588261553538781"
     },
     "user_tz": 420
    },
    "id": "bf23837a-4488-491a-af93-e31f95d3af11",
    "outputId": "267b3929-7943-4ed9-e540-29eacbd5765c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing data from zip: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following datasets:  ['hurricane-matthew', 'flooding-fire']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing data from zip: 100%|██████████| 3/3 [00:14<00:00,  4.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following datasets:  ['socal-fire', 'midwest-flooding', 'hurricane-matthew']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing data from zip: 100%|██████████| 3/3 [00:00<00:00, 442.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following datasets:  ['socal-fire', 'midwest-flooding', 'hurricane-matthew']\n",
      "CPU times: user 15.1 s, sys: 1.45 s, total: 16.5 s\n",
      "Wall time: 17.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set Date Source\n",
    "data_file = f'{data_dir}/sp24_grad_project_data.zip'\n",
    "\n",
    "# Load Test Images\n",
    "test_data_images = read_files_from_zip(data_file, r'(test)_images_([\\w-]*)\\.npz')\n",
    "\n",
    "# Load Train Images and Labels\n",
    "train_data_images = read_files_from_zip(data_file, r'satellite-image-data/([\\w-]*)/(train)_images\\.npz')\n",
    "train_data_labels = read_files_from_zip(data_file, r'satellite-image-data/([\\w-]*)/(train)_labels\\.npy')\n",
    "\n",
    "# Convert to Dataframe\n",
    "train_df = data2pd(train_data_images, train_data_labels)\n",
    "test_df = data2pd(test_data_images)\n",
    "\n",
    "# Clean up\n",
    "del(test_data_images)\n",
    "del(train_data_images)\n",
    "del(train_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30322dcf-29e5-4eaa-a583-0ebbc605331b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "editable": true,
    "executionInfo": {
     "elapsed": 736,
     "status": "error",
     "timestamp": 1712183688395,
     "user": {
      "displayName": "Brent Allen Thorne",
      "userId": "03585588261553538781"
     },
     "user_tz": 420
    },
    "id": "30322dcf-29e5-4eaa-a583-0ebbc605331b",
    "outputId": "5c1aa248-81ad-4440-d9e1-53d7d809db65",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label balance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "type       label\n",
       "fire       0        7204\n",
       "           3        1064\n",
       "           1          69\n",
       "           2          43\n",
       "flood      0        6734\n",
       "           1         114\n",
       "           2          97\n",
       "           3          59\n",
       "hurricane  1        5236\n",
       "           0        2631\n",
       "           3        1740\n",
       "           2        1544\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Train label balance:')\n",
    "show_balance(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f294d-1884-4f3f-9668-9d01001b2394",
   "metadata": {
    "id": "401f294d-1884-4f3f-9668-9d01001b2394"
   },
   "source": [
    "## Data Cleansing Plan\n",
    "\n",
    "There are many potential enhancements, this seems a reasonable starting point.\n",
    "\n",
    " - Scale images to 180x180 (ResNet50 default)\n",
    " - Normalize pixel values from 0-255 to be float from 0-1\n",
    " - For Type Classifier, encode level+type to float between -1 and 1\n",
    " - For Level Classifier, ohe level to catagories 0-4\n",
    " - When undersampling, sample without replacement within innerquartile\n",
    " - When oversampling, include entire set then sample with replacement and augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052d829-0e89-499e-8294-fe7df8570250",
   "metadata": {
    "id": "0052d829-0e89-499e-8294-fe7df8570250"
   },
   "source": [
    "\n",
    "### Model Task B\n",
    "\n",
    "Data split and augmentation plan (Disaster Level Classification)\n",
    "\n",
    "We'll use a mix of oversampling and undersampling...\n",
    " - oversample the minority class using replacement\n",
    " - undersample by randomly delete rows from the majority class to match\n",
    "\n",
    " - Validation:\n",
    "    - Sample without replacement N samples of each disaster level\n",
    " - Training:\n",
    "    - Select a sample size such that is a balance between drop/augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73d5e06d-bd20-44eb-8626-c8374d6a015a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1712183799621,
     "user": {
      "displayName": "Brent Allen Thorne",
      "userId": "03585588261553538781"
     },
     "user_tz": 420
    },
    "id": "73d5e06d-bd20-44eb-8626-c8374d6a015a",
    "outputId": "c8503a44-cce4-40c6-da96-813d9019cd79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'type', 'image', 'label', 'size'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size</th>\n",
       "      <td>26535.0</td>\n",
       "      <td>8169.871867</td>\n",
       "      <td>12364.717747</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1936.0</td>\n",
       "      <td>4340.0</td>\n",
       "      <td>10240.0</td>\n",
       "      <td>410464.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count         mean           std   min     25%     50%      75%  \\\n",
       "size  26535.0  8169.871867  12364.717747  84.0  1936.0  4340.0  10240.0   \n",
       "\n",
       "           max  \n",
       "size  410464.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "# Add a size column\n",
    "def get_size(img):\n",
    "    dim = img.shape\n",
    "    return dim[0] * dim[1]\n",
    "train_df['size'] = train_df['image'].apply(get_size)\n",
    "\n",
    "# Show size feature and range\n",
    "print(train_df.columns)\n",
    "train_df[['size']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed31d9a3-d81d-4683-bda7-06d84925d5d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1712183802468,
     "user": {
      "displayName": "Brent Allen Thorne",
      "userId": "03585588261553538781"
     },
     "user_tz": 420
    },
    "id": "ed31d9a3-d81d-4683-bda7-06d84925d5d3",
    "outputId": "61251673-e407-4a67-b51c-a900c5692652"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type       label\n",
       "hurricane  1        5236\n",
       "           0        2631\n",
       "           3        1740\n",
       "           2        1544\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create dataset for Disaster Type Classification\n",
    "\n",
    "# This dataset is a bit more balanced.\n",
    "classify_level_df = train_df[(train_df.type == 'hurricane')]\n",
    "show_balance(classify_level_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ed65cf4-d32a-4928-85ef-637fc64c2d34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "executionInfo": {
     "elapsed": 390,
     "status": "ok",
     "timestamp": 1712183807819,
     "user": {
      "displayName": "Brent Allen Thorne",
      "userId": "03585588261553538781"
     },
     "user_tz": 420
    },
    "id": "1ed65cf4-d32a-4928-85ef-637fc64c2d34",
    "outputId": "4cda7c6e-664a-40ae-88cc-4026523cf087",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "# Find a good validation split size\n",
    "# We'll pick a whole number for our puny human brains.\n",
    "validation_sample_size = int(classify_level_df.groupby('type')['label'].value_counts().min() * .1945)\n",
    "print(validation_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56d65fd1-98a7-47b2-9c6c-d82bf0974d55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1712183809648,
     "user": {
      "displayName": "Brent Allen Thorne",
      "userId": "03585588261553538781"
     },
     "user_tz": 420
    },
    "id": "56d65fd1-98a7-47b2-9c6c-d82bf0974d55",
    "outputId": "b794217f-7e3f-4078-94f9-fe01b28459c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "# Find a good augmentation size\n",
    "# We'll pick a whole number for our puny human brains.\n",
    "augmentation_size = int(classify_level_df.groupby('type')['label'].value_counts().min() * 1.2955)\n",
    "print(augmentation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca54a167-ee42-48b4-a7ee-b958c2f350d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1712183811693,
     "user": {
      "displayName": "Brent Allen Thorne",
      "userId": "03585588261553538781"
     },
     "user_tz": 420
    },
    "id": "ca54a167-ee42-48b4-a7ee-b958c2f350d1",
    "outputId": "ffaf18b7-8677-4883-f82a-6620873c23c8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  (11151, 5)\n",
      "After:  (9951, 5)\n"
     ]
    }
   ],
   "source": [
    "# Create validation set for modelling.\n",
    "\n",
    "# We would like for our test set to representive\n",
    "# of our population so we'll limit it\n",
    "# sizes in the innerquartile.\n",
    "classify_level_valid_df = pd.DataFrame()\n",
    "print('Before: ',classify_level_df.shape)\n",
    "for T in classify_level_df.type.unique():\n",
    "    for L in classify_level_df.label.unique():\n",
    "        #print(f'{T}:{L}')\n",
    "        sample_df =  classify_level_df[(classify_level_df['type'] == T) & (classify_level_df['label'] == L)\n",
    "        & ((classify_level_df['size'] <= 10240) & (classify_level_df['size'] >= 1936))\n",
    "        ].sample(validation_sample_size)\n",
    "        #display(sample_df.shape)\n",
    "        #print(type(sample_df))\n",
    "        classify_level_valid_df = pd.concat([sample_df, classify_level_valid_df])\n",
    "# drop our test sample from the parent df\n",
    "classify_level_df = classify_level_df.drop(index=classify_level_valid_df.index)\n",
    "\n",
    "print('After: ',classify_level_df.shape)\n",
    "\n",
    "assert(9951 == classify_level_df.shape[0]), 'Invalid parent data size, something looks fishy.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74ffb1ce-547f-46f3-88d6-aafd3410c4b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "editable": true,
    "executionInfo": {
     "elapsed": 337,
     "status": "ok",
     "timestamp": 1712183815551,
     "user": {
      "displayName": "Brent Allen Thorne",
      "userId": "03585588261553538781"
     },
     "user_tz": 420
    },
    "id": "74ffb1ce-547f-46f3-88d6-aafd3410c4b3",
    "outputId": "02517cfc-4536-445c-86d5-735fa771b49f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type       label\n",
       "hurricane  0        2000\n",
       "           1        2000\n",
       "           2        2000\n",
       "           3        2000\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from feature_utils import augment_image\n",
    "\n",
    "# Now augment data until balanced.\n",
    "# We'll use a mix of oversampling and undersampling...\n",
    "# - oversample the minority class using replacement\n",
    "# - undersample by randomly delete rows from the majority class to match\n",
    "\n",
    "classify_level_train_df = pd.DataFrame()\n",
    "\n",
    "# Set a reasonable argumentation limit\n",
    "sample_size = augmentation_size\n",
    "\n",
    "# We apply flip, rotate augmentation to the\n",
    "# sample with replacement samples.\n",
    "# We might duplicate an augmentation if the\n",
    "# same record is sampled and the augmentation\n",
    "# is applied.\n",
    "# Anyway... Cool thing is that we can get as many unique\n",
    "# balanced training sets as we desire.\n",
    "\n",
    "for T in classify_level_df.type.unique():\n",
    "    for L in classify_level_df.label.unique():\n",
    "        record_size = classify_level_df[\n",
    "        (classify_level_df['type'] == T) & (classify_level_df['label'] == L)].shape[0]\n",
    "        if (record_size > sample_size):\n",
    "            sample_df = classify_level_df[\n",
    "            (classify_level_df['type'] == T) & (classify_level_df['label'] == L)].sample(sample_size)\n",
    "        else:\n",
    "            sample_df = classify_level_df[\n",
    "            (classify_level_df['type'] == T) & (classify_level_df['label'] == L)]\n",
    "            classify_level_train_df = pd.concat([sample_df, classify_level_train_df])\n",
    "            sample_df = classify_level_df[\n",
    "            (classify_level_df['type'] == T) & (classify_level_df['label'] == L)].sample(\n",
    "                sample_size-record_size, replace=True)\n",
    "            sample_df['image'] = sample_df['image'].apply(augment_image)\n",
    "        classify_level_train_df = pd.concat([sample_df, classify_level_train_df])\n",
    "count_s = classify_level_train_df.groupby('type')['label'].value_counts()\n",
    "display(count_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea2246c7-5725-465b-9044-e952ff4a5a44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1712183821215,
     "user": {
      "displayName": "Brent Allen Thorne",
      "userId": "03585588261553538781"
     },
     "user_tz": 420
    },
    "id": "ea2246c7-5725-465b-9044-e952ff4a5a44",
    "outputId": "d0d36d60-937e-4763-8400-2a1e0b89f5af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type       label\n",
       "hurricane  0        300\n",
       "           1        300\n",
       "           2        300\n",
       "           3        300\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "type       label\n",
       "hurricane  0        2000\n",
       "           1        2000\n",
       "           2        2000\n",
       "           3        2000\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now let's recap our datasets for task B\n",
    "show_balance(classify_level_valid_df)\n",
    "show_balance(classify_level_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "455d6753-8d58-400b-9fb3-c9ec3830b0df",
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "aborted",
     "timestamp": 1712183688404,
     "user": {
      "displayName": "Brent Allen Thorne",
      "userId": "03585588261553538781"
     },
     "user_tz": 420
    },
    "id": "455d6753-8d58-400b-9fb3-c9ec3830b0df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300/2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647b7540-40ef-414e-98a6-20634619fb54",
   "metadata": {
    "id": "647b7540-40ef-414e-98a6-20634619fb54"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4436e110-a00e-4a2f-97e7-ad0c38146c0d",
   "metadata": {
    "id": "4436e110-a00e-4a2f-97e7-ad0c38146c0d"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9be15434-8525-40d0-8e1b-c82390509cee",
   "metadata": {
    "id": "9be15434-8525-40d0-8e1b-c82390509cee"
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "**Damage Level Classification**\n",
    " - The objective is to create a classifier that can automatically determine the level of building damage following a disaster, specifically for hurricanes.\n",
    "\n",
    "For now we've only added size and used it for undersampling.\n",
    "These are images so we'll let Gauss do his thing...\n",
    "...try not to second guess Gauss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f5976e-79d4-4c52-b7d1-634f3a4a357d",
   "metadata": {
    "id": "d8f5976e-79d4-4c52-b7d1-634f3a4a357d"
   },
   "source": [
    "## Modeling\n",
    "\n",
    "Let's cookup a the simplest model we can think of...\n",
    "For us this a autoencoder where we encoding our convolved image into a latent space then decode to our classes.\n",
    "\n",
    "**Task B**\n",
    " - Reshape ((180x180, normalize for positive definteness)\n",
    " - Conv2d (180x180)\n",
    " - MaxPooling2D (60x60, Dense, ReLu)\n",
    " - Dropout\n",
    " - Flatten\n",
    " - Dense (4 classes) levels 0-3\n",
    "\n",
    "Consider augment the data with 0-45 degree rotataton and 100to120% scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aaca417-de6c-4bd5-b4e6-8e0c209a25d8",
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "aborted",
     "timestamp": 1712183688405,
     "user": {
      "displayName": "Brent Allen Thorne",
      "userId": "03585588261553538781"
     },
     "user_tz": 420
    },
    "id": "9aaca417-de6c-4bd5-b4e6-8e0c209a25d8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from random import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a5490f-66ed-4bff-a7f1-7978814004be",
   "metadata": {
    "id": "89a5490f-66ed-4bff-a7f1-7978814004be"
   },
   "source": [
    "Encode Labels and Resize Images for modeling.\n",
    "\n",
    "Note: In general changing aspect ratios show little effect on the ablity of CNNs to learn.  We note changing the aspect ratio increases the number of epochs to train to some loss threshold.  This implies that for simpiler models might benifit from preserving the aspect ratio.\n",
    "\n",
    "Things to consider:\n",
    " - A model might memorize the low-dimension aspect-ratio rather than high-dimensional image-features.\n",
    " - Consider the balance of compute, memory and time it takes for model to converge.\n",
    " - Train with cropped large images and zero-background centered small images.\n",
    " - Make a more extractable model, for example use 2d convolution for each color then apply SVG. Plot labels using PCA dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4085ce1f-51fe-4557-8af8-6354e1ab0e5e",
   "metadata": {
    "id": "4085ce1f-51fe-4557-8af8-6354e1ab0e5e"
   },
   "source": [
    "### Model Task B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aa6c002-7d2f-4f8c-ae0b-2d0429353005",
   "metadata": {
    "executionInfo": {
     "elapsed": 2482,
     "status": "ok",
     "timestamp": 1712183940587,
     "user": {
      "displayName": "Brent Allen Thorne",
      "userId": "03585588261553538781"
     },
     "user_tz": 420
    },
    "id": "2aa6c002-7d2f-4f8c-ae0b-2d0429353005"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "image_dim = 180\n",
    "\n",
    "# Encode Labels\n",
    "def encode_level(df):\n",
    "    X = df['image'].to_list()\n",
    "    X = np.array([cv2.resize(img, (image_dim, image_dim)) for img in X])\n",
    "    #Y = to_categorical(df.label, num_classes=4)\n",
    "    df['level_code'] = df['label']\n",
    "    df['level_code'] = df['level_code']/df['level_code'].abs().max()\n",
    "    Y = df['level_code'].astype('float32') # let's keep it linear\n",
    "    return X,Y\n",
    "\n",
    "X_valid, Y_valid = encode_level(classify_level_valid_df)\n",
    "X_train, Y_train = encode_level(classify_level_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JDipOJJhHbh6",
   "metadata": {
    "id": "JDipOJJhHbh6"
   },
   "source": [
    "### New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "_wglVp37H3Vc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1712183946266,
     "user": {
      "displayName": "Brent Allen Thorne",
      "userId": "03585588261553538781"
     },
     "user_tz": 420
    },
    "id": "_wglVp37H3Vc",
    "outputId": "b01170dd-4424-469e-db56-f0a6cf42f23d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type       label\n",
       "hurricane  1        5236\n",
       "           0        2631\n",
       "           3        1740\n",
       "           2        1544\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This dataset is a bit more balanced.\n",
    "classify_level_df = train_df[(train_df.type == 'hurricane')]\n",
    "show_balance(classify_level_df)\n",
    "backend.clear_session()  # clear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "x5hw9-NpIBOz",
   "metadata": {
    "executionInfo": {
     "elapsed": 692,
     "status": "ok",
     "timestamp": 1712183948330,
     "user": {
      "displayName": "Brent Allen Thorne",
      "userId": "03585588261553538781"
     },
     "user_tz": 420
    },
    "id": "x5hw9-NpIBOz"
   },
   "outputs": [],
   "source": [
    "# Encode Labels\n",
    "\n",
    "image_dim = 72 # 180//3\n",
    "\n",
    "def encode_level(df):\n",
    "    X = df['image'].to_list()\n",
    "    X = np.array([cv2.resize(img, (image_dim, image_dim)) for img in X])\n",
    "    Y = to_categorical(df.label, num_classes=4)\n",
    "    return X,Y\n",
    "\n",
    "X_train, Y_train = encode_level(classify_level_df)\n",
    "\n",
    "del(classify_level_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "sxpaG_UvHZmE",
   "metadata": {
    "executionInfo": {
     "elapsed": 68043,
     "status": "aborted",
     "timestamp": 1712183688421,
     "user": {
      "displayName": "Brent Allen Thorne",
      "userId": "03585588261553538781"
     },
     "user_tz": 420
    },
    "id": "sxpaG_UvHZmE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/models/model_1_2_0\n",
      "../Data/models/model_1_2_0/history.pkl\n",
      "Gherkin injested.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Rescaling, BatchNormalization\n",
    "\n",
    "# Transfer model\n",
    "model_name = 'model_1_2_0'\n",
    "model, history, notes = load_model_data(model_name, path=data_dir)\n",
    "\n",
    "if model == None:   \n",
    "    # Define our number of splits\n",
    "    n_splits = 5\n",
    "    \n",
    "    # Create a StratifiedKFold object\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    \n",
    "    # Define our ImageDataGenerator for augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "    \n",
    "    # Loop over our folds\n",
    "    for train_index, test_index in skf.split(X_train, Y_train.argmax(axis=1)):\n",
    "        # Create the train and test sets\n",
    "        X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "        Y_train_fold, Y_test_fold = Y_train[train_index], Y_train[test_index]\n",
    "    \n",
    "        # Fit our ImageDataGenerator\n",
    "        datagen.fit(X_train_fold)\n",
    "    \n",
    "        model = Sequential([\n",
    "            Input(shape = (image_dim, image_dim, 3)),\n",
    "            BatchNormalization(),\n",
    "            Conv2D(filters = 16, kernel_size = 2, padding = \"same\", activation = \"relu\"),\n",
    "            MaxPooling2D(pool_size = 2),\n",
    "            Dropout(0.2),\n",
    "            Conv2D(filters = 32, kernel_size = 2, padding = \"same\", activation = \"relu\"),\n",
    "            MaxPooling2D(pool_size = 2),\n",
    "            Dropout(0.1),\n",
    "            Conv2D(filters = 32, kernel_size = 2, padding = \"same\", activation = \"relu\"),\n",
    "            MaxPooling2D(pool_size = 2),\n",
    "            Dropout(0.2),\n",
    "            Conv2D(filters = 32, kernel_size = 2, padding = \"same\", activation = \"relu\"),\n",
    "            MaxPooling2D(pool_size = 2),\n",
    "            Dropout(0.2),\n",
    "            Flatten(),\n",
    "            Dense(512, activation='relu'),\n",
    "            Dropout(0.4),\n",
    "             Dense(4, activation='linear')# Dense(4, activation='softmax')\n",
    "            ])\n",
    "    \n",
    "    \n",
    "        # Compile the model\n",
    "        #model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "    \n",
    "        # Train the model\n",
    "        model.fit(datagen.flow(X_train_fold, Y_train_fold), validation_data=(X_test_fold, Y_test_fold), epochs=10, batch_size=32,)\n",
    "    \n",
    "    save_model_data(model_name, model, None, 'trained on k=5 folds', path=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dw8TOyr9U8Pj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16736,
     "status": "ok",
     "timestamp": 1712184172028,
     "user": {
      "displayName": "Brent Allen Thorne",
      "userId": "03585588261553538781"
     },
     "user_tz": 420
    },
    "id": "dw8TOyr9U8Pj",
    "outputId": "30d79065-1fed-4799-ac5b-28a97c5399f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349/349 [==============================] - 8s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_train)\n",
    "actual = Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "SVaRyd46VB0w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1712184174609,
     "user": {
      "displayName": "Brent Allen Thorne",
      "userId": "03585588261553538781"
     },
     "user_tz": 420
    },
    "id": "SVaRyd46VB0w",
    "outputId": "59f51df8-9529-4caa-cb41-e84e06aed366"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1474989825663362"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(actual, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "K5I76DCOXskC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15710,
     "status": "ok",
     "timestamp": 1712184191732,
     "user": {
      "displayName": "Brent Allen Thorne",
      "userId": "03585588261553538781"
     },
     "user_tz": 420
    },
    "id": "K5I76DCOXskC",
    "outputId": "7c232fcf-2a37-48f4-9a69-7d82c580e308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349/349 [==============================] - 7s 21ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.51      0.53      2631\n",
      "           1       0.67      0.46      0.54      5236\n",
      "           2       0.35      0.52      0.42      1544\n",
      "           3       0.48      0.79      0.60      1740\n",
      "\n",
      "    accuracy                           0.53     11151\n",
      "   macro avg       0.51      0.57      0.52     11151\n",
      "weighted avg       0.57      0.53      0.53     11151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "def print_metrics(model, X, Y):\n",
    "    Y_pred = model.predict(X)\n",
    "    Y_pred_classes = np.argmax(Y_pred, axis = 1)\n",
    "    Y_true = np.argmax(Y, axis = 1)\n",
    "\n",
    "    # compute the confusion matrix\n",
    "    print(classification_report(Y_true, Y_pred_classes, target_names = ['0', '1', '2', '3']))\n",
    "\n",
    "print_metrics(model, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "95fee9ed-6823-4c46-99dd-fc857cb2e4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "#test_df\n",
    "level_df = test_df[(test_df.type == 'hurricane')]\n",
    "X = level_df['image'].to_list()\n",
    "X = np.array([cv2.resize(img, (image_dim, image_dim)) for img in X])\n",
    "Y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6a345a2b-32fb-42f4-a11c-1443ecb82bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_activation(y):\n",
    "    return (y*4).astype(int)\n",
    "    \n",
    "Y_pred_custom = custom_activation(Y_pred) #.reshape(-1,)\n",
    "Y_pred_custom.min(), Y_pred_custom.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "186c79a3-af46-461e-acf6-6406f9798477",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_custom= Y_pred.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "57c231a7-1538-47ec-b73c-691dac5d98ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values: [0 1 2 3]\n",
      "Counts: [613 898 557 720]\n"
     ]
    }
   ],
   "source": [
    "# Get unique values and their counts\n",
    "unique_values, counts = np.unique(Y_pred_custom, return_counts=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Unique values:\", unique_values)\n",
    "print(\"Counts:\", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "542d9e6c-1875-4170-bc7e-63674b38aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_out = pd.DataFrame(Y_pred_custom, columns=['pred'])\n",
    "Y_pred_out.to_csv('test_images_hurricane-matthew_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7b1e5fed-a611-4342-a5ba-701f0ff5ca47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2783</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2785</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2787</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2788 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pred\n",
       "0        2\n",
       "1        2\n",
       "2        2\n",
       "3        0\n",
       "4        0\n",
       "...    ...\n",
       "2783     0\n",
       "2784     3\n",
       "2785     2\n",
       "2786     2\n",
       "2787     1\n",
       "\n",
       "[2788 rows x 1 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dea9a78-1d90-4386-9194-8f816d68758d",
   "metadata": {
    "id": "1dea9a78-1d90-4386-9194-8f816d68758d"
   },
   "source": [
    "## Notes and Resources\n",
    "\n",
    "https://learningds.org/ch/19/class_pred.html\n",
    "\n",
    "https://neptune.ai/blog/keras-loss-functions\n",
    "\n",
    "https://www.h2kinfosys.com/blog/linear-classifier-with-tensorflow-keras/\n",
    "\n",
    "Deep Learning with Python, by François Chollet\n",
    "\n",
    "https://github.com/fchollet/deep-learning-with-python-notebooks/\n",
    "\n",
    "Python Data Science Handbook, by Jake VanderPlas\n",
    "\n",
    "https://github.com/jakevdp/PythonDataScienceHandbook"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1S0Dc0AbT5dXdi8W-Fry9PkXnfdbNdCnQ",
     "timestamp": 1712180460297
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
