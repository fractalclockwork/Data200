{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3e59Togx1Dz"
   },
   "source": [
    "# EDA (Topic 1: Computer Vision)\n",
    "\n",
    "## Overview\n",
    "\n",
    "### Project Overview\n",
    "\n",
    " This project aims to leverage data science techniques to aid emergency response efforts in disaster situations. The goal is to provide quick and accurate information about affected areas, enhancing the effectiveness of the response.\n",
    "\n",
    "### Goals\n",
    "\n",
    "The project has three main objectives:\n",
    "\n",
    " - To gain experience working with image data, specifically through the use of common feature extraction techniques such as Sobel edge filtering.\n",
    " - To handle real-world data complexities, including class imbalance, low signal-to-noise ratio, and high-dimensional data.\n",
    " - To design effective preprocessing and featurization pipelines for tackling challenging machine learning tasks.\n",
    "\n",
    "### Mission\n",
    "\n",
    " The mission of this project is to expedite the process of annotating high-resolution satellite images, which are crucial for disaster response. The challenge is to develop a computer vision approach that can automatically assign labels to these images.\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    " The dataset comprises satellite images of buildings post various natural disasters. The buildings are labeled based on the extent of damage sustained, with a scale ranging from 0 (no damage) to 3 (destroyed).\n",
    "\n",
    "### Tasks\n",
    "\n",
    "**Disaster Type Classification**\n",
    " -  The goal is to build a classifier that can automatically categorize images based on the type of disaster scenario, such as flooding or fire.\n",
    "   \n",
    "**Damage Level Classification**\n",
    " - The objective is to create a classifier that can automatically determine the level of building damage following a disaster, specifically for hurricanes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Resources and Documentation\n",
    "\n",
    "[Project Decription and Requirements](https://ds100.org/sp24/gradproject/)\n",
    "\n",
    "[Checkpoint 1 Peer Review](https://docs.google.com/document/d/1OsxAc2tLlMfjUKo6kqLkjSFyGHAkNw0dYwyCxPJRM0Y/edit?usp=sharing)\n",
    "\n",
    "[EDA Report](https://docs.google.com/document/d/1uGEexKdfgWnqgpCZdw4mDDLdXdmsvOgFUIYb103U8fk/edit?usp=sharing)\n",
    "\n",
    "[EDA Q&A](https://docs.google.com/document/d/1FoMKI6U4TKebRYRNB5MQRUHUG6Q2LmopVlLTjIEYu0w/edit?usp=sharing)\n",
    "\n",
    "[Source](https://github.com/fractalclockwork/Data200/tree/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wTGbKtAIRKH_"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m_jjjkF3V6JT",
    "outputId": "d6583610-f0e4-4b7d-a101-77ef8367a37c"
   },
   "outputs": [],
   "source": [
    "# For Colab we define a path for local libraries and files \n",
    "# this sort defeats the reasoning behind using a JSON config file,\n",
    "# think about a better way to do this, oh well\n",
    "COLAB = False\n",
    "\n",
    "# mount Colab drive and set library path\n",
    "if COLAB: \n",
    "  import sys\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')   \n",
    "  path = '/content/drive/My Drive/Colab Notebooks/grad_project'\n",
    "  #data_dir = f'{path}/data/sp24_grad_project_data/satellite-image-data'\n",
    "  sys.path.insert(0, path) # for colab to see local libraries\n",
    "else:\n",
    "  path = '.'\n",
    "  #data_dir = f'{path}/../Data/sp24_grad_project_data/satellite-image-data'\n",
    "\n",
    "figure_path = f'{path}/../Figures'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tN4EZPFzRKID"
   },
   "source": [
    "## Loading the data\n",
    "To get started let's first load in the data! We will make use of the helper functions provided in `data_utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "h0YxG3ynRKIH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 03:00:19.996301: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-25 03:00:19.998867: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-25 03:00:20.033912: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-25 03:00:20.748145: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from data_utils import get_images, get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgZagYE1RKIJ"
   },
   "source": [
    "The following is the list of disasters in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "v0TcvASbRKIK"
   },
   "outputs": [],
   "source": [
    "disaster_list = [\"hurricane-matthew\", \"socal-fire\", \"midwest-flooding\"]\n",
    "test_disaster_list = [\"hurricane-matthew\", \"flooding-fire\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SMJ8XCuRKIM"
   },
   "source": [
    "We will load each disaster train dataset into a dictionary. Recall that each disaster consists of images and labels (0 - 3) of the damage level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vr8oQR_RRKIN",
    "outputId": "5c511e7d-40af-4429-ec88-2f2dc2f1040e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train images and labels for hurricane-matthew dataset...\n",
      "\tLoaded 11151 records.\n",
      "Loading train images and labels for socal-fire dataset...\n",
      "\tLoaded 8380 records.\n",
      "Loading train images and labels for midwest-flooding dataset...\n",
      "\tLoaded 7004 records.\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "split = \"train\"\n",
    "\n",
    "# this is not a useful abstraction atm\n",
    "with open(f'{path}/config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "    if COLAB:\n",
    "      data_dir = config['colab_data_dir']\n",
    "    else:\n",
    "      data_dir = config['data_dir']\n",
    "\n",
    "for disaster in disaster_list:\n",
    "    print(f\"Loading {split} images and labels for {disaster} dataset...\")\n",
    "    images = get_images(data_dir, disaster, split=split)\n",
    "    labels = get_labels(data_dir, disaster, split=split)\n",
    "    data[disaster] = {\"images\": images, \"labels\": labels}\n",
    "    print(f\"\\tLoaded {len(labels)} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vr8oQR_RRKIN",
    "outputId": "5c511e7d-40af-4429-ec88-2f2dc2f1040e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test images for hurricane-matthew test dataset...\n",
      "\tLoaded 2788  records.\n",
      "Loading test images for flooding-fire test dataset...\n",
      "\tLoaded 3847  records.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from data_utils import load_images\n",
    "\n",
    "split = 'test'\n",
    "test_data = {}\n",
    "for disaster in test_disaster_list:\n",
    "    print(f\"Loading {split} images for {disaster} test dataset...\")\n",
    "    images_path = os.path.join(data_dir,  f\"test_images_{disaster}.npz\")\n",
    "    \n",
    "    images = load_images(images_path)\n",
    "    test_data[disaster] = images\n",
    "    print(f\"\\tLoaded {len(images)}  records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../Data/sp24_grad_project_data/satellite-image-data\u001b[00m\n",
      "├── \u001b[01;34mhurricane-matthew\u001b[00m\n",
      "│   ├── train_images.npz\n",
      "│   └── train_labels.npy\n",
      "├── \u001b[01;34mmidwest-flooding\u001b[00m\n",
      "│   ├── train_images.npz\n",
      "│   └── train_labels.npy\n",
      "├── \u001b[01;34msocal-fire\u001b[00m\n",
      "│   ├── train_images.npz\n",
      "│   └── train_labels.npy\n",
      "├── test_images_flooding-fire.npz\n",
      "└── test_images_hurricane-matthew.npz\n",
      "\n",
      "3 directories, 8 files\n"
     ]
    }
   ],
   "source": [
    "!tree {data_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "We can see that the files for this dataset have already been split and use the path for disaster type, a file prefix to identify the split(test/train), and a file suffix to identify datatype(image/label).  Let's also consider how we might develop a Class for our EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dataframe**\n",
    "\n",
    "Let's create a dataframe with all our data so that we can explore it more easily.  For the sake of clarity and conciseness we'll create a library for our EDA utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>label</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>image_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26535.000000</td>\n",
       "      <td>26535.000000</td>\n",
       "      <td>26535.000000</td>\n",
       "      <td>26535.000000</td>\n",
       "      <td>26535.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4590.139099</td>\n",
       "      <td>0.654833</td>\n",
       "      <td>78.507933</td>\n",
       "      <td>80.090183</td>\n",
       "      <td>8169.871867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2837.802075</td>\n",
       "      <td>1.000181</td>\n",
       "      <td>49.831279</td>\n",
       "      <td>52.705180</td>\n",
       "      <td>12364.717747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2211.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>1936.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4422.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>4340.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6633.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>10240.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11150.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>716.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>410464.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                idx         label        height         width     image_size\n",
       "count  26535.000000  26535.000000  26535.000000  26535.000000   26535.000000\n",
       "mean    4590.139099      0.654833     78.507933     80.090183    8169.871867\n",
       "std     2837.802075      1.000181     49.831279     52.705180   12364.717747\n",
       "min        0.000000      0.000000      3.000000      3.000000      84.000000\n",
       "25%     2211.000000      0.000000     43.000000     44.000000    1936.000000\n",
       "50%     4422.000000      0.000000     66.000000     66.000000    4340.000000\n",
       "75%     6633.000000      1.000000    104.000000    104.000000   10240.000000\n",
       "max    11150.000000      3.000000    716.000000    890.000000  410464.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Our library is underdevelopment, thus we reload as we add features.\n",
    "import eda_utils\n",
    "from importlib import reload\n",
    "reload(eda_utils)\n",
    "\n",
    "df = eda_utils.data2df(disaster_list, data)\n",
    "assert (df.shape[0] == 26535), 'Invalid dataframe length!'\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_color</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[75.7022792022792, 105.27578347578347, 101.084...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[88.45159914712154, 119.42288557213931, 110.24...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[111.97156084656085, 122.16071428571429, 119.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[96.97969722693831, 128.50396151669497, 127.72...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[87.17985125084516, 108.7789046653144, 96.3113...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26530</th>\n",
       "      <td>[90.0832342449465, 117.02497027348394, 109.146...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26531</th>\n",
       "      <td>[95.70530626780626, 125.3482905982906, 122.0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26532</th>\n",
       "      <td>[64.78615552325581, 90.17532703488372, 77.7979...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26533</th>\n",
       "      <td>[47.41594827586207, 76.15110837438424, 59.3152...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26534</th>\n",
       "      <td>[58.76563396442914, 82.8995983935743, 69.27165...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26535 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               avg_color  label\n",
       "0      [75.7022792022792, 105.27578347578347, 101.084...      3\n",
       "1      [88.45159914712154, 119.42288557213931, 110.24...      0\n",
       "2      [111.97156084656085, 122.16071428571429, 119.0...      1\n",
       "3      [96.97969722693831, 128.50396151669497, 127.72...      0\n",
       "4      [87.17985125084516, 108.7789046653144, 96.3113...      2\n",
       "...                                                  ...    ...\n",
       "26530  [90.0832342449465, 117.02497027348394, 109.146...      0\n",
       "26531      [95.70530626780626, 125.3482905982906, 122.0]      0\n",
       "26532  [64.78615552325581, 90.17532703488372, 77.7979...      0\n",
       "26533  [47.41594827586207, 76.15110837438424, 59.3152...      0\n",
       "26534  [58.76563396442914, 82.8995983935743, 69.27165...      0\n",
       "\n",
       "[26535 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['avg_color','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fire :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <td>8380.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8379.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>8380.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>8380.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>509.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>width</th>\n",
       "      <td>8380.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>890.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_size</th>\n",
       "      <td>8380.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>289250.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count   min       max\n",
       "idx         8380.0   0.0    8379.0\n",
       "label       8380.0   0.0       3.0\n",
       "height      8380.0   4.0     509.0\n",
       "width       8380.0   3.0     890.0\n",
       "image_size  8380.0  98.0  289250.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flood :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <td>7004.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7003.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>7004.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>7004.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>716.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>width</th>\n",
       "      <td>7004.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>863.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_size</th>\n",
       "      <td>7004.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>410464.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count    min       max\n",
       "idx         7004.0    0.0    7003.0\n",
       "label       7004.0    0.0       3.0\n",
       "height      7004.0    3.0     716.0\n",
       "width       7004.0    6.0     863.0\n",
       "image_size  7004.0  126.0  410464.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hurricane :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <td>11151.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>11151.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>11151.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>354.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>width</th>\n",
       "      <td>11151.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>396.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_size</th>\n",
       "      <td>11151.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>110484.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count   min       max\n",
       "idx         11151.0   0.0   11150.0\n",
       "label       11151.0   0.0       3.0\n",
       "height      11151.0   3.0     354.0\n",
       "width       11151.0   4.0     396.0\n",
       "image_size  11151.0  84.0  110484.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tolal :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <td>26535.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>26535.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>26535.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>716.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>width</th>\n",
       "      <td>26535.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>890.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_size</th>\n",
       "      <td>26535.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>410464.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count   min       max\n",
       "idx         26535.0   0.0   11150.0\n",
       "label       26535.0   0.0       3.0\n",
       "height      26535.0   3.0     716.0\n",
       "width       26535.0   3.0     890.0\n",
       "image_size  26535.0  84.0  410464.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "type = ['fire', 'flood', 'hurricane']\n",
    "\n",
    "for t in type:\n",
    "    print(t,':')\n",
    "    display(df[df.disaster_type == t].describe().T[['count', 'min','max']])\n",
    "\n",
    "print('tolal',':')\n",
    "display(df.describe().T[['count', 'min','max']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Distribution\n",
    "\n",
    "Let's explore our labels to understand their distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {figure_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label count by disaster type\n",
    "\n",
    "dd = df # in case we want a different sample\n",
    "label_counts = dd.groupby('disaster_type')['label'].value_counts()\n",
    "#total_counts = label_counts.groupby(level=0).sum()\n",
    "#label_ratios = label_counts / total_counts\n",
    "\n",
    "palette = ['red', 'blue', 'green', 'orange']\n",
    "color_dict = {3: 'red', 0: 'green', 1: 'yellow', 2: 'orange'}\n",
    "\n",
    "# Pivot the data for plotting\n",
    "pivot_df = label_counts.unstack(fill_value=0)\n",
    "display(pivot_df)\n",
    "print(pivot_df)\n",
    "\n",
    "# side-by-side\n",
    "pivot_df.plot(kind='bar',  color=color_dict)\n",
    "plt.xlabel('Disaster Type')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Disaster Damage Level Counts by Disaster')\n",
    "#plt.legend(title='Damage Level Label',loc=(1.1,.5))\n",
    "plt.grid()\n",
    "plt.xticks(rotation=0)\n",
    "plt.savefig(f'{figure_path}/eda_label_totals_sidebyside.png', dpi=96)\n",
    "plt.show()\n",
    "\n",
    "# slacked\n",
    "pivot_df.plot(kind='bar', stacked=True, color=color_dict)\n",
    "plt.xlabel('Disaster Type')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Disaster Damage Level Counts by Disaster')\n",
    "#plt.legend(title='Damage Level Label')\n",
    "plt.grid()\n",
    "plt.xticks(rotation=0)\n",
    "plt.savefig(f'{figure_path}/eda_label_totals.png', dpi=96)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "We are dealing with a heavily imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label Ratios**\n",
    "\n",
    "Since we have such heavily imbalanced dataset let's take a look at the relative ratios of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.shape[0] == 26535), 'Invalid dataframe length!'\n",
    "\n",
    "dfs = [df, df[df.label > 0]]\n",
    "for i, dd in enumerate(dfs):\n",
    "    label_counts = dd.groupby('disaster_type')['label'].value_counts()\n",
    "    total_counts = label_counts.groupby(level=0).sum()\n",
    "    label_ratios = label_counts / total_counts\n",
    "\n",
    "    palette = ['red', 'blue', 'green', 'orange']\n",
    "    color_dict = {3: 'red', 0: 'green', 1: 'yellow', 2: 'orange'}\n",
    "    \n",
    "    # Pivot the data for plotting\n",
    "    pivot_df = label_ratios.unstack(fill_value=0)\n",
    "    #pivot_df = label_counts.unstack(fill_value=0)\n",
    "    \n",
    "    pivot_df.plot(kind='bar', stacked=True, color=color_dict)\n",
    "    #display(pivot_df)\n",
    "    plt.xlabel('Disaster Type')\n",
    "    plt.ylabel('Ratios of Label Counts')\n",
    "    plt.legend(title='Damage Level')\n",
    "    plt.xticks(rotation=0)\n",
    "    if i==0:\n",
    "        plt.title('Disaster Damage Level Ratios (All)')\n",
    "        plt.savefig(f'{figure_path}/eda_label_ratios.png', dpi=96)\n",
    "    else:\n",
    "        plt.title('Disaster Damage Level Ratios (Level > 0)')\n",
    "        plt.savefig(f'{figure_path}/eda_label_ratios_excluded.png', dpi=96)\n",
    "\n",
    "    plt.show()\n",
    "assert (df.shape[0] == 26535), 'Invalid dataframe length!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "We can clearly we are dealing with a unbalanced dataset, the label distribution varies across disaster types.  This might be expected as fire damage might tend towards heavier damage, while flooding damage is limited to low lying thus tends to less damage, and finally where hurricane damage might be expected to be more varied due to combined factors such as flood and wind damage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Size Distribution\n",
    "\n",
    "Let's explore the range of image sizes in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe()[['height','width','image_size']].T[['min','max','mean']].apply(round))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.shape[0] == 26535), 'Invalid dataframe length!'\n",
    "alpha = 0.8\n",
    "sns.histplot(data=df[df.disaster_type== 'fire'], x='image_size', kde=True, stat=\"count\", label='image size', log_scale=True, color = 'red', alpha=alpha)\n",
    "sns.histplot(data=df[df.disaster_type== 'flood'], x='image_size', kde=True, stat=\"count\", label='image size', log_scale=True, color = 'grey', alpha=alpha)\n",
    "sns.histplot(data=df[df.disaster_type== 'hurricane'], x='image_size', kde=True, stat=\"count\", label='image size', log_scale=True, color = 'lightblue', alpha=alpha)\n",
    "plt.legend(labels=['fire','flood','hurricane'])\n",
    "plt.xlabel(\"Image Size (log scale)\");\n",
    "plt.title(\"Image Size Counts by Disaster Type\");\n",
    "plt.savefig(f'{figure_path}/eda_image_size_hist.png', dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.shape[0] == 26535), 'Invalid dataframe length!'\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(10, 5))\n",
    "sns.boxplot(data=df, x=\"width\", ax=ax[0], log_scale=False);\n",
    "sns.boxplot(data=df, x=\"height\", ax=ax[1], log_scale=False);\n",
    "sns.boxplot(data=df, x=\"image_size\", ax=ax[2], log_scale=True);\n",
    "\n",
    "plt.suptitle('Distribution of image dimensions and size (log scale)')\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{figure_path}/eda_image_size_box.png', dpi=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Size Damage Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(\"bright\") # This only set the kde curve color...\n",
    "color_dict = { 0: 'green', 1: 'yellow', 2: 'orange', 3: 'red'}\n",
    "\n",
    "alpha = 1\n",
    "#plt.figure(figsize=(15, 10))\n",
    "share='all'\n",
    "fig, ax = plt.subplots(3,1, sharex=share, sharey=share, figsize=(15,10))\n",
    "for axi, type in enumerate(df.disaster_type.unique()):\n",
    "    #plt.subplot(3, 1, axi+1, sharex='all', sharey='all')\n",
    "    for level in range(4):\n",
    "        #sns.histplot(data=df[(df.disaster_type== type) & (df.label== level)], x='image_size', kde=True, stat=\"count\", label=f'{level}', log_scale=True, color = color_dict[level], alpha=alpha)\n",
    "        sns.kdeplot(data=df[(df.disaster_type== type) & (df.label== level)], x='image_size', label=f'{level}', log_scale=True, color = color_dict[level], alpha=alpha, bw_adjust=.9, ax=ax[axi])\n",
    "    ax[axi].legend()\n",
    "    ax[axi].set_xlabel('');\n",
    "    ax[axi].set_ylabel(f'{type.title()}')\n",
    "    ax[axi].set_title(f'{type.title()}')\n",
    "    ax[axi].grid()\n",
    "    #plt.gca().set_yscale('log')\n",
    "\n",
    "fig.supxlabel(\"Image Size (log scale)\")\n",
    "fig.supylabel('Frequency Density')\n",
    "plt.suptitle(\"Frequency Density of Image Size by Disaster Type and Level\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'{figure_path}/eda_image_size_kde_bylevel.png', dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_palette(\"bright\") # This only set the kde curve color...\n",
    "color_dict = { 0: 'green', 1: 'yellow', 2: 'orange', 3: 'red'}\n",
    "\n",
    "alpha = 0.6\n",
    "\n",
    "share='all'\n",
    "fig, ax = plt.subplots(3,1, sharex=share, sharey=share, figsize=(15,10))\n",
    "for axi, type in enumerate(df.disaster_type.unique()):\n",
    "    #plt.subplot(3, 1, axi+1, sharex='all', sharey='all')\n",
    "    for level in range(4):\n",
    "        subdf = df[(df.disaster_type== type) & (df.label== level)]\n",
    "        sns.histplot(data=subdf, x=subdf['image_size'], kde=False, stat=\"count\", label=f'{level}', log_scale=False, color = color_dict[level], alpha=alpha, ax=ax[axi])\n",
    "    ax[axi].legend()\n",
    "    ax[axi].set_xlabel('')\n",
    "    ax[axi].set_ylabel(f'{type.title()}')\n",
    "    ax[axi].set_title(f'{type.title()}')\n",
    "    ax[axi].grid()\n",
    "    plt.gca().set_yscale('log')\n",
    "fig.supxlabel('Image Size (linear scale)')#,y=0)\n",
    "fig.supylabel('Counts')#, x=0)\n",
    "plt.suptitle(\"Image Size Counts by Disaster Level\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'{figure_path}/eda_image_size_hist_bylevel.png', dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note there are very few images for type/level, flood/3.\n",
    "30/(df[(df.disaster_type == 'flood')& (df.label == 3)].label.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Now do color\n",
    "\n",
    "# note: we might use replacement but we have compute and memory limits already, so smaller the better\n",
    "sample_size = 30 # find a repesentive sample size %13,  \n",
    "print(f'Using a sample size of: {sample_size}')\n",
    "\n",
    "\n",
    "frames = []\n",
    "for disaster_type in df.disaster_type.unique():\n",
    "    for label in df.label.unique():\n",
    "        #frames.append(df[(df.disaster_type == disaster_type) & (df.label == label)].sample(int(sample_size)))\n",
    "        frames.append(df[(df.disaster_type == disaster_type) & (df.label == label)].sample(int(sample_size)))   \n",
    "color_df = pd.concat(frames)\n",
    "color_df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = []\n",
    "dd = color_df\n",
    "\n",
    "for i in range(len(dd)):\n",
    "    label = dd.iloc[i]['label']\n",
    "    type_ = dd.iloc[i]['disaster_type']\n",
    "    img = dd.iloc[i]['img']\n",
    "    for color, color_name in zip(range(3), ['Red', 'Green', 'Blue']):\n",
    "        pixels = img[:, :, color].ravel()\n",
    "        data.extend([[label, type_, color_name, pixel] for pixel in pixels])\n",
    "\n",
    "# Create a new DataFrame\n",
    "plot_df = pd.DataFrame(data, columns=['Label', 'Type', 'Color', 'Intensity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create a color palette that matches the color names\n",
    "palette = {'Red': 'r', 'Green': 'g', 'Blue': 'b'}\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "#sns.boxplot(x='Label', y='Intensity', hue='Color', data=plot_df, palette=palette)\n",
    "sns.catplot(x=\"Label\", y=\"Intensity\", hue=\"Color\", col=\"Type\", data=plot_df, kind=\"box\", height=4, aspect=.7, palette=palette)\n",
    "\n",
    "plt.suptitle('Distribution of Color Intensities Grouped by Type and Label',y=1)\n",
    "\n",
    "plt.savefig(f'{figure_path}/eda_image_color_by_level.png', dpi=96)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Color Anaylsis\n",
    "\n",
    "<using Shirley's code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Red\"] = df[\"avg_color\"].apply(lambda x: x[0])\n",
    "df[\"Green\"] = df[\"avg_color\"].apply(lambda x: x[1])\n",
    "df[\"Blue\"] = df[\"avg_color\"].apply(lambda x: x[2])\n",
    "\n",
    "melt_df = pd.melt(df, id_vars=[\"disaster_type\", \"label\"], value_vars=[\"Red\", \"Green\", \"Blue\"], var_name=\"color_channel\", value_name=\"color_val\")\n",
    "\n",
    "melt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"ticks\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(melt_df, x=\"disaster_type\", y=\"color_val\", hue=\"color_channel\", palette=[\"r\", \"g\", \"b\"])\n",
    "plt.xlabel('Disaster Type')\n",
    "plt.ylabel('Average Pixel Intensities')\n",
    "plt.title('Average Color Values by Disaster Type')\n",
    "plt.legend(title='Color Channel', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{figure_path}/eda_avg_pixel_bar.png', dpi=96)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, sharex=True, sharey=True, figsize=(30,10))\n",
    "\n",
    "sns.boxplot(melt_df[melt_df[\"label\"] == 0], x=\"disaster_type\", y=\"color_val\", hue=\"color_channel\", palette=[\"r\", \"g\", \"b\"], ax=ax[0])\n",
    "ax[0].set_xlabel(\"\")\n",
    "ax[0].set_ylabel(\"\")\n",
    "ax[0].set_title(\"No Damage\")\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "ax[0].legend([])\n",
    "\n",
    "sns.boxplot(melt_df[melt_df[\"label\"] == 1], x=\"disaster_type\", y=\"color_val\", hue=\"color_channel\", palette=[\"r\", \"g\", \"b\"], ax=ax[1])\n",
    "ax[1].set_xlabel(\"\")\n",
    "ax[1].set_ylabel(\"\")\n",
    "ax[1].set_title(\"Minor Damage\")\n",
    "ax[1].legend([])\n",
    "\n",
    "sns.boxplot(melt_df[melt_df[\"label\"] == 2], x=\"disaster_type\", y=\"color_val\", hue=\"color_channel\", palette=[\"r\", \"g\", \"b\"], ax=ax[2])\n",
    "ax[2].set_xlabel(\"\")\n",
    "ax[2].set_ylabel(\"\")\n",
    "ax[2].set_title(\"Major Damage\")\n",
    "ax[2].legend([])\n",
    "\n",
    "sns.boxplot(melt_df[melt_df[\"label\"] == 3], x=\"disaster_type\", y=\"color_val\", hue=\"color_channel\", palette=[\"r\", \"g\", \"b\"], ax=ax[3])\n",
    "ax[3].set_xlabel(\"\")\n",
    "ax[3].set_ylabel(\"\")\n",
    "ax[3].set_title(\"Destroyed\")\n",
    "ax[3].legend([])\n",
    "\n",
    "fig.text(0.5, 0.00, 'Disaster Type', ha='center', fontsize=16)\n",
    "fig.text(0.00, 0.5, 'Average Pixel Intensities', va='center', rotation='vertical', fontsize=16)\n",
    "plt.legend(handles, labels, title='Color Channel', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.suptitle('Average Color Values by Damage Level', fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{figure_path}/eda_avg_pixel_bar_bydamage.png', dpi=96)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, sharex=True, sharey=True, figsize=(30,10))\n",
    "\n",
    "sns.boxplot(melt_df[melt_df[\"disaster_type\"] == \"hurricane\"], x=\"label\", y=\"color_val\", hue=\"color_channel\", palette=[\"r\", \"g\", \"b\"], ax=ax[0])\n",
    "ax[0].set_xlabel(\"\")\n",
    "ax[0].set_ylabel(\"\")\n",
    "ax[0].set_xticks([0, 1, 2, 3], [\"No Damage\", \"Minor Damage\", \"Major Damage\", \"Destroyed\"])\n",
    "ax[0].set_title(\"Hurricane\")\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "ax[0].legend([])\n",
    "\n",
    "sns.boxplot(melt_df[melt_df[\"disaster_type\"] == \"fire\"], x=\"label\", y=\"color_val\", hue=\"color_channel\", palette=[\"r\", \"g\", \"b\"], ax=ax[1])\n",
    "ax[1].set_xlabel(\"\")\n",
    "ax[1].set_ylabel(\"\")\n",
    "ax[1].set_title(\"Fire\")\n",
    "ax[1].legend([])\n",
    "\n",
    "sns.boxplot(melt_df[melt_df[\"disaster_type\"] == \"flood\"], x=\"label\", y=\"color_val\", hue=\"color_channel\", palette=[\"r\", \"g\", \"b\"], ax=ax[2])\n",
    "ax[2].set_xlabel(\"\")\n",
    "ax[2].set_ylabel(\"\")\n",
    "ax[2].set_title(\"Flood\")\n",
    "ax[2].legend([])\n",
    "\n",
    "fig.text(0.5, 0.00, 'Damage Level', ha='center', fontsize=16)\n",
    "fig.text(0.00, 0.5, 'Average Pixel Intensities', va='center', rotation='vertical', fontsize=16)\n",
    "plt.legend(handles, labels, title='Color Channel', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.suptitle('Average Color Values by Damage Level', fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{figure_path}/eda_avg_pixel_bar_bytype.png', dpi=96)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, sharex=True, sharey=True, figsize=(30,10))\n",
    "\n",
    "sns.boxplot(melt_df[melt_df[\"disaster_type\"] == \"hurricane\"], x=\"label\", y=\"color_val\", hue=\"color_channel\", palette=[\"r\", \"g\", \"b\"], ax=ax[0])\n",
    "ax[0].set_xlabel(\"\")\n",
    "ax[0].set_ylabel(\"\")\n",
    "ax[0].set_xticks([0, 1, 2, 3], [\"No Damage\", \"Minor Damage\", \"Major Damage\", \"Destroyed\"])\n",
    "ax[0].set_title(\"Hurricane\")\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "ax[0].legend([])\n",
    "\n",
    "sns.boxplot(melt_df[melt_df[\"disaster_type\"] == \"fire\"], x=\"label\", y=\"color_val\", hue=\"color_channel\", palette=[\"r\", \"g\", \"b\"], ax=ax[1])\n",
    "ax[1].set_xlabel(\"\")\n",
    "ax[1].set_ylabel(\"\")\n",
    "ax[1].set_title(\"Fire\")\n",
    "ax[1].legend([])\n",
    "\n",
    "sns.boxplot(melt_df[melt_df[\"disaster_type\"] == \"flood\"], x=\"label\", y=\"color_val\", hue=\"color_channel\", palette=[\"r\", \"g\", \"b\"], ax=ax[2])\n",
    "ax[2].set_xlabel(\"\")\n",
    "ax[2].set_ylabel(\"\")\n",
    "ax[2].set_title(\"Flood\")\n",
    "ax[2].legend([])\n",
    "\n",
    "fig.text(0.5, 0.00, 'Damage Level', ha='center', fontsize=16)\n",
    "fig.text(0.00, 0.5, 'Average Pixel Intensities', va='center', rotation='vertical', fontsize=16)\n",
    "plt.legend(handles, labels, title='Color Channel', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.suptitle('Average Color Values by Damage Level', fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{figure_path}/eda_avg_pixel_bar_bytype.png', dpi=96)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df[\"std_color\"] = df[\"img\"].apply(lambda x: np.std(np.array(x), axis=(0, 1)))\n",
    "\n",
    "df[\"Red\"] = df[\"std_color\"].apply(lambda x: x[0])\n",
    "df[\"Green\"] = df[\"std_color\"].apply(lambda x: x[1])\n",
    "df[\"Blue\"] = df[\"std_color\"].apply(lambda x: x[2])\n",
    "\n",
    "var_df = pd.melt(df, id_vars=[\"disaster_type\", \"label\"], value_vars=[\"Red\", \"Green\", \"Blue\"], var_name=\"color_channel\", value_name=\"std_pixel\")\n",
    "\n",
    "var_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"ticks\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(var_df, x=\"disaster_type\", y=\"std_pixel\", hue=\"color_channel\", palette=[\"r\", \"g\", \"b\"])\n",
    "plt.xlabel('Disaster Type')\n",
    "plt.ylabel('Pixel Intensity Standard Deviation')\n",
    "plt.title('Standard Deviation of Color Values by Disaster Type')\n",
    "plt.legend(title='Color Channel', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{figure_path}/eda_std_pixel_bar.png', dpi=96)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, sharex=True, sharey=True, figsize=(30,10))\n",
    "\n",
    "sns.boxplot(var_df[var_df[\"label\"] == 0], x=\"disaster_type\", y=\"std_pixel\", hue=\"color_channel\", palette=[\"r\", \"g\", \"b\"], ax=ax[0])\n",
    "ax[0].set_xlabel(\"\")\n",
    "ax[0].set_ylabel(\"\")\n",
    "ax[0].set_title(\"No Damage\")\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "ax[0].legend([])\n",
    "\n",
    "sns.boxplot(var_df[var_df[\"label\"] == 1], x=\"disaster_type\", y=\"std_pixel\", hue=\"color_channel\", palette=[\"r\", \"g\", \"b\"], ax=ax[1])\n",
    "ax[1].set_xlabel(\"\")\n",
    "ax[1].set_ylabel(\"\")\n",
    "ax[1].set_title(\"Minor Damage\")\n",
    "ax[1].legend([])\n",
    "\n",
    "sns.boxplot(var_df[var_df[\"label\"] == 2], x=\"disaster_type\", y=\"std_pixel\", hue=\"color_channel\", palette=[\"r\", \"g\", \"b\"], ax=ax[2])\n",
    "ax[2].set_xlabel(\"\")\n",
    "ax[2].set_ylabel(\"\")\n",
    "ax[2].set_title(\"Major Damage\")\n",
    "ax[2].legend([]) \n",
    "\n",
    "sns.boxplot(var_df[var_df[\"label\"] == 3], x=\"disaster_type\", y=\"std_pixel\", hue=\"color_channel\", palette=[\"r\", \"g\", \"b\"], ax=ax[3])\n",
    "ax[3].set_xlabel(\"\")\n",
    "ax[3].set_ylabel(\"\")\n",
    "ax[3].set_title(\"Destroyed\")\n",
    "ax[3].legend([])\n",
    "\n",
    "fig.text(0.5, 0.00, 'Disaster Type', ha='center', fontsize=16)\n",
    "fig.text(0.00, 0.5, 'Pixel Intensity Standard Deviation', va='center', rotation='vertical', fontsize=16)\n",
    "plt.legend(handles, labels, title='Color Channel', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.suptitle('Standard Deviation of Color Values by Damage Level', fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{figure_path}/eda_std_pixel_bar_bydamage.png', dpi=96)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, sharex=True, sharey=True, figsize=(30,10))\n",
    "\n",
    "sns.boxplot(var_df[var_df[\"disaster_type\"] == \"hurricane\"], x=\"label\", y=\"std_pixel\", hue=\"color_channel\", palette=[\"r\", \"g\", \"b\"], ax=ax[0])\n",
    "ax[0].set_xlabel(\"\")\n",
    "ax[0].set_ylabel(\"\")\n",
    "ax[0].set_xticks([0, 1, 2, 3], [\"No Damage\", \"Minor Damage\", \"Major Damage\", \"Destroyed\"])\n",
    "ax[0].set_title(\"Hurricane\")\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "ax[0].legend([])\n",
    "\n",
    "sns.boxplot(var_df[var_df[\"disaster_type\"] == \"fire\"], x=\"label\", y=\"std_pixel\", hue=\"color_channel\", palette=[\"r\", \"g\", \"b\"], ax=ax[1])\n",
    "ax[1].set_xlabel(\"\")\n",
    "ax[1].set_ylabel(\"\")\n",
    "ax[1].set_title(\"Fire\")\n",
    "ax[1].legend([])\n",
    "\n",
    "sns.boxplot(var_df[var_df[\"disaster_type\"] == \"flood\"], x=\"label\", y=\"std_pixel\", hue=\"color_channel\", palette=[\"r\", \"g\", \"b\"], ax=ax[2])\n",
    "ax[2].set_xlabel(\"\")\n",
    "ax[2].set_ylabel(\"\")\n",
    "ax[2].set_title(\"Flood\")\n",
    "ax[2].legend([])\n",
    "\n",
    "fig.text(0.5, 0.00, 'Damage Level', ha='center', fontsize=16)\n",
    "fig.text(0.00, 0.5, 'Pixel Intensity Standard Deviation', va='center', rotation='vertical', fontsize=16)\n",
    "plt.legend(handles, labels, title='Color Channel', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.suptitle('Standard Deviation of Color Values by Damage Level', fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{figure_path}/eda_std_pixel_bar_bytype.png', dpi=96)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create \"heatmap\" of average pixel value for each category\n",
    "heat_array = [] \n",
    "label = []\n",
    "dis_type = [\"hurricane\", \"flood\", \"fire\"]\n",
    "dmg_labels = [\"None\", \"Minor\", \"Major\", \"Destroyed\"]\n",
    "for dtype in dis_type:\n",
    "    for i in range(4):\n",
    "        type_vals = df[(df[\"disaster_type\"] == dtype) & (df[\"label\"] == i)][\"avg_color\"].sample(n=16).values\n",
    "        type_vals = np.array([np.array(i) for i in type_vals]).reshape((16, 3))\n",
    "        heat_array.append(type_vals)\n",
    "        label.append(dmg_labels[i])\n",
    "heat_array = np.array(heat_array)\n",
    "heat_array = heat_array.reshape((12, 16, 3))\n",
    "print(np.array(heat_array).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(heat_array.astype(int))\n",
    "\n",
    "for border_coord in range(12):\n",
    "    if border_coord == 4 or border_coord == 8:\n",
    "        plt.axhline(border_coord - 0.5, color='white', linewidth=5)\n",
    "    else:\n",
    "        plt.axhline(border_coord - 0.5, color='white', linewidth=1)\n",
    "\n",
    "ax.axes.xaxis.set_ticks([])\n",
    "ax.set_yticks(np.arange(12), label)\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "plt.xlabel(\"Sampled Average Image Color\", fontsize=16)\n",
    "plt.title(\"Average Image Color per Disaster and Damage Type\", fontsize=18)\n",
    "fig.text(-0.05, 0.5, 'Image Type', va='center', rotation='vertical', fontsize=16)\n",
    "fig.text(-0.015, 0.25, 'Fire', va='center', rotation='vertical', fontsize=14)\n",
    "fig.text(-0.015, 0.5, 'Flood', va='center', rotation='vertical', fontsize=14)\n",
    "fig.text(-0.015, 0.75, 'Hurricane', va='center', rotation='vertical', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{figure_path}/eda_avg_color_swatch.png', dpi=96)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "We can see that there is a wide range of image dimensions and sizes.  From the paper describing the dataset we might expect that larger images show a larger area and that smaller images are a polygon representing a building.  We can also see that the distribution of image sizes varies depending on the disaster type.  We might consider how we might normalize the images sizes, for example we might crop larger images or center smaller images on background(average pixel color or black).  We might also rotate and crop images to address label imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Analysis\n",
    "\n",
    "In addition to statistical analysis, we shall visualize a few samples to gave a general understanding of our data set.  We've created a library to ease this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some functions to aid our exploration of images\n",
    "assert (df.shape[0] == 26535), 'Invalid dataframe length!'\n",
    "\n",
    "import eda_utils\n",
    "from importlib import reload \n",
    "reload(eda_utils) \n",
    "from eda_utils import show_df\n",
    "\n",
    "show_df(df, seed=1001, share=False, title='Image Sample (from complete dataset)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "We can see that we are dealing with a wide range of image sizes and aspect ranges.  Also, we can see from this simple sample that distribution of label is now equal.  This is precisely what we expect our analysis of size and label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall image size distribution\n",
    "display(df[['image_size','width','height']].describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are filtering low aspect ratio image to make visul comparison between disaster types and levels easier.  \n",
    "This also allows use to use shared x and y scales in our subplots.  This is gain a general understanding of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = 104 * 2\n",
    "upper = np.inf\n",
    "show_df(df[(df['width']>lower) & (df['width']<upper) &\n",
    "        (df['height']>lower) & (df['height']<upper)], seed=5707, share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(dd, seed=None, share=False, title=None):\n",
    "    random_state = seed\n",
    "    \n",
    "    disaster_types = ['fire','flood','hurricane']\n",
    "    rows, cols = len(disaster_types),4\n",
    "    fig, ax = plt.subplots(rows,cols, sharex=share, sharey=share, figsize=(10,3*rows))\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    for i in range(rows):  # type index\n",
    "        for j in range(cols): # label index\n",
    "            axk = ax[i][j]\n",
    "            #axk.plot(1,1,marker='o') # debug\n",
    "            dd = df[(df['disaster_type']== disaster_types[i]) & (df['label']== j)].sample(1)\n",
    "            img = dd.iloc[0].img\n",
    "            axk.imshow(img.astype(np.uint8))\n",
    "            if i == 2:\n",
    "                axk.set_xlabel(f'{j}')\n",
    "            if j == 0:\n",
    "                axk.set_ylabel(f'{disaster_types[i]}')\n",
    "    #plt.subplots_adjust(wspace=0.2,hspace=0.2)\n",
    "    #plt.tight_layout()\n",
    "    fig.align_xlabels()\n",
    "    fig.align_ylabels()\n",
    "    fig.supxlabel('Damage Level')#,y=0)\n",
    "    fig.supylabel('Damage Type')#, x=0)\n",
    "       \n",
    "show_sample(df,seed = 1, title='Image Samples of each Disaster Type and Damage Level')\n",
    "plt.savefig(f'{figure_path}/eda_image_sample_each_type_label.png', dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by (filtered around median aspect ratio)\n",
    "upper = 104\n",
    "lower = 43\n",
    "subframe = df[\n",
    "        (df['width']>lower) & (df['width']<upper) &\n",
    "        (df['height']>lower) & (df['height']<upper)]\n",
    "\n",
    "show_sample(subframe,seed = 11, title='Image Samples of each Disaster Type and Damage Level (filtered around median aspect ratio)')\n",
    "#plt.savefig(f'{figure_path}/eda_image_sample_each_type_label_common_aspect.png', dpi=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "A comparative visual analysis was performed using the entire dataset sampled for each disaster type and damage level. \n",
    "\n",
    "The dataset appers to contain cropped smaller images detailing areas from a set of larger area images.\n",
    "There are some black pixel artifacts in the larger image that also appear in the cropped images.\n",
    "\n",
    "It can be seen that some of the cropped images show the same area shifted.\n",
    "Some images are cropped with a high aspect ratio to detail a specific structure.\n",
    "All images appear to be of a same scale and resolution, however there appears to be some blurring, possibly due to geometry transform.\n",
    "\n",
    "There are shadows in present in all disaster types and levels however the satelite images were taken at or near local noon.\n",
    "The fire and flood images show some occultation due to smoke and clouds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a sample of images\n",
    "#show_df(df, seed = None)\n",
    "\n",
    "# Show Color Distribution\n",
    "#eda_utils.plot_color_distribution(img)\n",
    "#eda_utils.get_color_density(df.sample(1))\n",
    "#eda_utils.show_color_density(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix i. Image Analysis\n",
    "\n",
    "Some examples for image visualization and analysis where provided.  We'll retained these as a reference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ifDAfCzRKIP"
   },
   "source": [
    "## Visualize an image and its label in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "GwAe13yHRKIR",
    "outputId": "ff9fb2e5-31e2-44d1-b23e-5fe998d0d4bc"
   },
   "outputs": [],
   "source": [
    "assert (df.shape[0] == 26535), 'Invalid dataframe length!'\n",
    "disaster = disaster_list[0]\n",
    "images = data[disaster][\"images\"]\n",
    "labels = data[disaster][\"labels\"]\n",
    "\n",
    "# index of the image to display\n",
    "idx = 0\n",
    "img = images[idx]\n",
    "label = labels[idx]\n",
    "\n",
    "plt.title(f\"{disaster}, image {idx}, label {label}\")\n",
    "plt.imshow(img.astype(np.uint8))\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# show shape of image\n",
    "print(f\"image shape: {img.shape}\")\n",
    "assert (df.shape[0] == 26535), 'Invalid dataframe length!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EK3ZidNRKIU"
   },
   "source": [
    "## Extracting Color Information.\n",
    "Each image is an RGB image represented as a numpy array of dimensions ~~(width, height, 3)~~ (ncols, nrows, 3) which represent (image_height, image_width, colors). The last dimension corresponds to the RGB *color channels* in the image. We can split the image by color channels and visualize each of them individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 983
    },
    "id": "pDQmH-9_RKIV",
    "outputId": "c37d0898-23ab-4715-f8a7-65528844974d"
   },
   "outputs": [],
   "source": [
    "assert (df.shape[0] == 26535), 'Invalid dataframe length!'\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(img)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(img[:, :, 0], cmap=\"Reds\", vmin=0, vmax=255)\n",
    "plt.title(\"Red Channel\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(img[:, :, 1], cmap=\"Greens\", vmin=0, vmax=255)\n",
    "plt.title(\"Green Channel\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(img[:, :, 2], cmap=\"Blues\", vmin=0, vmax=255)\n",
    "plt.title(\"Blue Channel\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "assert (df.shape[0] == 26535), 'Invalid dataframe length!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJgP21I-RKIV"
   },
   "source": [
    "We can zoom in on just a pixel as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "I0WMn9taRKIW",
    "outputId": "9a9fcd67-98c4-4e18-c007-e31935a133f6"
   },
   "outputs": [],
   "source": [
    "assert (df.shape[0] == 26535), 'Invalid dataframe length!'\n",
    "coord = (15, 35)\n",
    "pixel_value = img[coord[0], coord[1], :]\n",
    "pixel_image = np.array([[pixel_value]])\n",
    "\n",
    "plt.imshow(pixel_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Pixel value: {pixel_value}, coordinate: {coord}\")\n",
    "assert (df.shape[0] == 26535), 'Invalid dataframe length!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1djquawRKIX"
   },
   "source": [
    "## Extracting Edge and Texture Information\n",
    "\n",
    "So far we have considered the **color** information in an image. However, other important aspects of an image are the edges and texture.\n",
    "\n",
    "Let us consider some popular feature processing methods for extracting edge and texture information. We will use the utilities provided in `feature_utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGochWj5RKIY"
   },
   "outputs": [],
   "source": [
    "from feature_utils import get_sobel_features, get_gabor_features, generate_gabor_kernel, get_local_binary_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHnSAYdURKIY"
   },
   "source": [
    "Let's try the Sobel Edge Filter.\n",
    "\n",
    "Read more about Sobel edge detection: https://en.wikipedia.org/wiki/Sobel_operator and https://scikit-image.org/docs/stable/auto_examples/edges/plot_edge_filter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "v3xEe3XnRKIZ",
    "outputId": "ef02c161-a17a-4563-9293-a8c52565042c"
   },
   "outputs": [],
   "source": [
    "edges = get_sobel_features(img)\n",
    "plt.imshow(edges, cmap=\"gray\")\n",
    "plt.title(\"Sobel Edge Detection\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOXp4GNmRKIZ"
   },
   "source": [
    "Now let's look at local binary patterns.\n",
    "\n",
    "Read more about LBP: https://en.wikipedia.org/wiki/Local_binary_patterns and https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_local_binary_pattern.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "F_sT-geSRKIa",
    "outputId": "b4a9b25c-cac8-4b4c-f01a-7b979dcbdfa9"
   },
   "outputs": [],
   "source": [
    "lbp = get_local_binary_pattern(img, radius=3)\n",
    "plt.imshow(lbp, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Local Binary Pattern (LBP)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJ-LZUoXRKIb"
   },
   "source": [
    "Another popular filter for image processing is the Gabor filter. The Gabor filters are a family of filters parametrized by orientation, frequency, bandwith, etc. Let's generate one such filter.\n",
    "\n",
    "Read more about Gabor filters: https://en.wikipedia.org/wiki/Gabor_filter and https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_gabor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CUMsDfbRKIb"
   },
   "outputs": [],
   "source": [
    "theta = 0\n",
    "sigma = 1.0\n",
    "frequency = 0.1\n",
    "\n",
    "kernel = generate_gabor_kernel(theta, sigma, frequency)\n",
    "gabor = get_gabor_features(img, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntEEzTtuRKIc"
   },
   "source": [
    "Let's visualize this filter (aka kernel) and the response of the image when we apply the filter. Concretely we are taking the convolution of the image with the filter. See https://en.wikipedia.org/wiki/Kernel_(image_processing) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "oOoO7lelRKId",
    "outputId": "e6907e98-5b40-449d-b593-de5214e95c2d"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))  # 1 row, 2 columns\n",
    "\n",
    "# Plot Gabor Kernel\n",
    "axs[0].imshow(kernel, cmap=\"gray\")\n",
    "axs[0].axis(\"off\")  # Remove axis\n",
    "axs[0].set_title(\"Gabor Kernel\")\n",
    "\n",
    "# Plot Gabor Kernel Response\n",
    "axs[1].imshow(gabor, cmap=\"gray\")\n",
    "axs[1].axis(\"off\")  # Remove axis\n",
    "axs[1].set_title(\"Gabor Kernel Response\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLl3RlS9vpJz"
   },
   "source": [
    "# Appendix 0. Resources\n",
    "\n",
    "Grad project overview: <https://ds100.org/sp24/gradproject/>\n",
    "\n",
    "Paper on dataset: <https://arxiv.org/pdf/1911.09296.pdf>\n",
    "\n",
    "Complete dataset: <https://xview2.org/>\n",
    "\n",
    "Dealing with Imbalanced Classification: https://www.mdpi.com/2078-2489/14/1/54/pdf?version=1673866802\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ITg0dUlwIFp"
   },
   "source": [
    "# Appendix 1. Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8-8YdlowFMr",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h3 id=\"topic-1-computer-vision\">Topic 1: Computer Vision</h3> <p>In disaster situations, it is important for emergency response efforts to have access to quick and accurate information about an area in order to respond effectively. This project will explore how data science techniques can be useful for such efforts.</p> <h4 class=\"no_toc\" id=\"project-goals\">Project Goals</h4> <ul> <li>Learn to work with image data by learning to use common feature extraction techniques like Sobel edge filtering.</li> <li>Learn to work on real-world data with common complexities such as class imbalance, low signal-to-noise ratio, and high dimensional data.</li> <li>Learn how to design effective preprocessing and featurization pipelines for solving difficult machine learning tasks.</li> </ul> <h4 class=\"no_toc\" id=\"mission\">Mission</h4> <p>You have been hired by a crisis response agency to help assist them with your impressive data science skills! The agency has found that using satellite imagery is highly useful for supplying information for their response efforts. Unfortunately, however, annotating these high-resolution images can be a slow process for analysts. Your mission is to help address this challenge by developing an automatic computer vision approach!</p> <h4 class=\"no_toc\" id=\"dataset-description\">Dataset Description</h4> <p>The agency would like you to develop your approach on their internal dataset, derived from the <a href=\"https://xview2.org/\" target=\"_blank\">xView2 Challenge Dataset</a>. This dataset contains satellite images of buildings after various natural disasters. The buildings are labeled based on the level of damage sustained on a scale ranging from 0 (no damage) to 3 (destroyed).</p> <p>You can access all of the data within the <code class=\"language-plaintext highlighter-rouge\">./satellite-image-data</code> directory. The dataset consists of the following folders for different natural disasters</p> <ol> <li><code class=\"language-plaintext highlighter-rouge\">midwest-flooding</code></li> <li><code class=\"language-plaintext highlighter-rouge\">socal-fire</code></li> <li><code class=\"language-plaintext highlighter-rouge\">hurricane-matthew</code></li> </ol> <p>Within each folder is a zip file <code class=\"language-plaintext highlighter-rouge\">train_images.npz</code> containing the satellite images as numpy arrays and a <code class=\"language-plaintext highlighter-rouge\">train_labels.npy</code> file with corresponding damage level labels.</p> <blockquote> <p>Testing: In the main directory, there are also the <code class=\"language-plaintext highlighter-rouge\">test_images_hurricane-matthew.npz</code> and <code class=\"language-plaintext highlighter-rouge\">test_images_flooding-fire.npz</code> zip files. The first contains test images from the <code class=\"language-plaintext highlighter-rouge\">hurricane-matthew</code> disaster and the latter consists of a combination of test images from <code class=\"language-plaintext highlighter-rouge\">midwest-flooding</code> and <code class=\"language-plaintext highlighter-rouge\">socal-fire</code>.</p> </blockquote> <h4 class=\"no_toc\" id=\"getting-started\">Getting Started</h4> <p>To help you with onboarding, the agency has provided a starter notebook <a href=\"https://data100.datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2FDS-100%2Fsp24-student&amp;urlpath=lab%2Ftree%2Fsp24-student%2Fgrad-proj%2Fcv-satellite-images%2Fstarter.ipynb&amp;branch=main\" target=\"_blank\"><code class=\"language-plaintext highlighter-rouge\">starter.ipynb</code></a> which will introduce you to the dataset and some useful internal tools. After completing the onboarding assignment you will be comfortable with the following:</p> <ol> <li>Loading and visualizing data using tools from <code class=\"language-plaintext highlighter-rouge\">data_utils.py</code></li> <li>Processing different color channels in the dataset images.</li> <li>Extracting feature information from images using tools from <code class=\"language-plaintext highlighter-rouge\">feature_utils.py</code>.</li> </ol> <h4 class=\"no_toc\" id=\"exploratory-data-analysis\">Exploratory Data Analysis</h4> <p>Now that you have successfully been onboarded, the agency would like you to start performing some exploratory data analysis to build an initial understanding of the data. As part of the exploratory data analysis, the agency is interested in understanding certain aspects of the dataset better. Specifically, they are looking for:</p> <ul> <li>Basic statistics about the dataset, such as the number of images per disaster type and the distribution of image sizes and damage labels.</li> <li>Insights into useful image features for classifying images based on disaster type or damage level. Previous interns have found color information to be potentially useful. You are tasked with verifying this and exploring whether color features can effectively differentiate: <ul> <li><code class=\"language-plaintext highlighter-rouge\">midwest-flooding</code> from <code class=\"language-plaintext highlighter-rouge\">socal-fire</code> images.</li> <li>Damage levels 1 and 3 within the <code class=\"language-plaintext highlighter-rouge\">hurricane-matthew</code> dataset.</li> </ul> </li> </ul> <p>Please prepare an EDA report to present to the agency leadership with the above in mind.</p> <h4 class=\"no_toc\" id=\"project-tasks\">Project Tasks</h4> <p>Now that leadership is pleased with your initial EDA report and confident in your data science ability, they would like you to assist the agency with various tasks. <em>Please complete Task A first and then Task B.</em></p> <h4 class=\"no_toc\" id=\"task-a-disaster-type-classification\"><em>Task A: Disaster Type Classification</em></h4> <p>The agency consists of different subdivisions for assisting with different disaster types, e.g., fires, floods, etc. In the event of a disaster, the agency mounts its response effort by first assessing the type of disaster and then requesting the appropriate subdivision to assist with the disaster.</p> <p>Your task is to assist the agency with making this initial call quickly by automatically classifying images based on the disaster scenario. Specifically, your role will be to build a classifier that can distinguish images from the <code class=\"language-plaintext highlighter-rouge\">midwest-flooding</code> disaster and the <code class=\"language-plaintext highlighter-rouge\">socal-fire</code> disaster.</p> <p>To assess your performance, please submit predictions for the <code class=\"language-plaintext highlighter-rouge\">test_images_flooding-fire.npz</code> images. This should be in a csv file <code class=\"language-plaintext highlighter-rouge\">test_images_flooding-fire_predictions.csv</code> consisting of a single column with no header, with a 0 to indicate a <code class=\"language-plaintext highlighter-rouge\">midwest-flooding</code> prediction and a 1 to indicate a <code class=\"language-plaintext highlighter-rouge\">socal-fire</code> prediction. The prediction in row <em>i</em> should correspond to the <em>ith</em> image.</p> <h4 class=\"no_toc\" id=\"task-b-damage-level-classification\"><em>Task B: Damage Level Classification</em></h4> <p>The agency needs to know how severe a disaster is in order to allocate resources for a response effectively. The agency is especially concerned with human lives and uses building damage as an important metric for disaster severity.</p> <p>Your task is to assist the agency by automatically detecting the building damage level after a disaster. Specifically, create a damage level classifier for the <code class=\"language-plaintext highlighter-rouge\">hurricane-matthew</code> disaster.</p> <p>To assess your performance, please submit predictions for the <code class=\"language-plaintext highlighter-rouge\">test_images_hurricane-matthew.npz</code> images. This should be in a CSV file <code class=\"language-plaintext highlighter-rouge\">test_images_hurricane-matthew_predictions.csv</code> consisting of a single column with no header, with a 0-3 prediction of the damage level. The prediction in row <em>i</em> should correspond to the <em>i</em>th image.</p> <h4 class=\"no_toc\" id=\"resources\">Resources</h4> <p>To assist you in your efforts the agency has compiled the following list of resources:</p> <ul> <li>For more background about the dataset you can look at the <a href=\"https://arxiv.org/pdf/1911.09296.pdf\" target=\"_blank\">paper</a> associated with the dataset.</li> <li>For image processing, <a href=\"https://scikit-image.org/\" target=\"_blank\">scikit-image</a> is a very useful library. This <a href=\"https://www.kaggle.com/code/bextuychiev/full-tutorial-on-image-processing-in-skimage\" target=\"_blank\">tutorial</a> may be helpful for learning how to use the library.</li> <li>For problems with imbalanced classes, the <a href=\"https://imbalanced-learn.org/stable/index.html\" target=\"_blank\">imblearn</a> library has useful tools and examples.</li> </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ThPGJdewk_Y"
   },
   "source": [
    "# Appendix 2. Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhgbYcApwXtD"
   },
   "source": [
    "<h2 id=\"group-formation--research-proposal\">Checkpoint 0. Group Formation + Research Proposal (complete)</h2> \n",
    "<p>The first deliverable of your group project is just to form your group, choose a dataset, and submit your implementation plan to <a href=\"https://forms.gle/DcBp3ZbM8TpTfSRD6\" target=\"_blank\">this google form</a> by 11:59 pm on 3/15. The implementation plan should consist of a series of steps for completing the project along with a timeline. You may form groups of 2 or 3 people with any Data 200/200A/200S student.</p> <!-- ## Checkpoint 1: EDA + Internal Peer Review The checkpoint is intended to keep you on track to meet your project goals. You will need to submit exploratory data analysis results on Gradescope. This will include submitting both a report of your results so far as well as all code necessary to replicate your results. Your submission should include: - **Project Introduction and Goals:** Please briefly introduce your project. Think about introducing your project to someone who has a background in data science but does not know the dataset and your research question. This part should not exceed 500 words. Here are some components to help you get started: - What is the dataset about? How was the data collected? What are the available features and information? What is the size of the dataset? - What questions do you plan to ask about the dataset? Why do we care about such a problem? - What is your workflow for the project? Your first step, second step… - What are the models you plan to use? Why would the model be a good fit for your project? What are potential pitfalls you could run into? - What is your goal for the project? What are the expected deliverables? - **EDA:** Show the results from your EDA work. You should include: - **Data Sampling and Collection** - How was the data collected? - Was there any potential bias introduced in the sampling process? - **Data Cleaning** - What type of data are you currently exploring? - What is the granularity of the data? - What does the distribution of the data look like? Are there any outliers? Are there any missing or invalid entries? - **Exploratory Data Analysis** - Is there any correlation between the variables you are interested in exploring? - How would you cleanly and accurately visualize the relationship among variables? - What are your EDA questions? (For example, are there any relationships between A and B? What is the distribution of A?). - Do you need to perform data transformations? - **Figures(tables, plots, etc.)** - Descriptions of your figures. Takeaways from the figures. - These figures must be of good quality (i.e. they must include axes, titles, labels, etc) and they must be relevant to your proposed analysis. - **Other Preliminary Results (optional)**: Please optionally post any other preliminary results here for our information. ## Checkpoint 2: Mandatory Check-In The purpose of this checkpoint is to ensure you are making progress and on schedule to submit the first draft of the project in 2 weeks time. You will be required to make a one-page document summarizing all of your progress so far, and you will have to bring the document to a one-on-one meeting with a staff member. Please look at the <a href=\"#checkpoint-2-mandatory-check-in-75\">rubric</a> for the checkpoint and what you need to include in the <a href=\"#final-project-report\">Final Project Report</a> when determining what to include in your one-page document; the document should be a brief summary of all your progress so far. The staff member will quickly skim the document and give you guidance on the project as a whole. More details about submitting the one-page document and signing up for the staff member meeting will be announced on Ed soon. <!-- ## Final Project Report The project submission should include the following two components, as well as the YouTube video recording (more information to be announced later). --> <!-- ### [Component 1] Analysis Notebooks This component includes all the Jupyter Notebook(s) containing all the analyses that you performed on the datasets to support your claims in your write-up. Make sure that all references to datasets are done as `data/[path to data files]`. By running these notebooks, we should be able to replicate all the analysis/figures done in your write-up. Your analysis notebook(s) should address all of the following components in the data science lifecycle. Please note that a thorough explanation of your thought process and approach is **as important as** your work. Unreadable/uncommented code will lose points. Along with the code for the EDA portion (which also has to be included), we have provided a few additional preliminary questions/tips you can consider for the modeling portion of the project: - What are the research questions that you are answering through your analysis? What type of machine learning problem are you investigating? - Which model(s) do you use and why? - How do you use your data for training and testing? - Does your model require hyperparameter tuning? If so, how do you approach it? - How do you engineer the features for your model? What are the rationales behind selecting these features? - How do you perform cross-validation on your model? - What loss metrics are you using to evaluate your model? Why? - From a bias-variance tradeoff standpoint, how do you assess the performance of your model? How do you check if it is overfitting? - How would you improve your model based on the outcome? - Are there any further extensions to your model that would be worth exploring? ### [Component 2] Project Write-Up This is a single PDF that summarizes your workflow and what you have learned. It should be structured as a research paper and include a title, list of authors, abstract, introduction, description of data, methodology, summary of results, discussion, conclusion, and references. Make sure to number figures and tables, include informative captions, and ensure you include the provenance of the figures in the main narrative. We encourage you to render the PDF using LaTeX, but we will not be able to provide assistance with LaTeX-related issues. Specifically, you should ensure you address the following in the narrative: * Clearly state the research questions and why they are interesting and important. * Introduction: ensure you include a brief survey of related work on the topic(s) of your analysis. Be sure to reference current approaches/research in the context of your project, as well as how your project differs from or complements existing research. You must cite all the references you discuss in this section. * Description of data: ensure you outline the summary of the data and how the data was prepared for the modeling phase (summarizing your EDA work). If applicable, descriptions of additional datasets that you gathered to support your analysis may also be included. * Methodology: carefully describe the methods/models you use and why they are appropriate for answering your research questions. You must include a detailed description of how modeling is done in your project, including inference or prediction methods used, feature engineering and regularization if applicable, and cross-validation or test data as appropriate for model selection and evaluation. You may also include interesting findings involving your datasets. * Summary of results: analyze your findings in relation to your research question(s). Include/reference visualizations and specific results. Discuss any interesting findings from your analysis. You are encouraged to compare the results using different inference or prediction methods (e.g. linear regression, logistic regression, or classification and regression trees). Can you explain why some methods performed better than others? * Discussion: evaluate your approach and discuss any limitations of the methods you used. Also, briefly describe any surprising discoveries and whether there are any interesting extensions to your analysis. The narrative PDF should include figures sparingly to support specific claims. It can include a few runnable code components, but it should not have large amounts of code. The length of the report should be 8 ± 2 pages when it is printed as a PDF, excluding figures and code. Tip: if you need to write a large amount of LaTeX on markdown, you may want to use the `%%latex` cell magic. However, we also encourage you to explore [Overleaf](https://www.overleaf.com) for easily writing clean LaTeX documents. Please submit everything as a zip file to the final report submission portal on Gradescope. Please make sure the folder in the zip file has the following structure: ``` [your studentIDs joined by _]/ data/[all datasets used] analysis/[analysis notebooks] narrative/[narrative PDF] figures/[figures included in the narrative PDF] ``` Please use student IDs joined by `_` as the name for the top-level directory. The analysis notebooks must be runnable within this directory structure. If the narrative PDF includes any figures that are created in the analysis notebooks, the figures should be saved to `figures/` by the analysis notebooks. --> <h2 id=\"rubrics\">Rubrics</h2> <p>This section includes a rubric for how different project deliverables are going to be graded. This section will be updated as we get further along the project timeline.</p> <h3 id=\"group-formation--research-proposal-5\">Group formation + Research Proposal (5%)</h3> <ul> <li>Short paragraph description of implementation plan and timeline (2%).</li> <li>Forming teams by the deadline (3%).</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "<h2 id=\"checkpoint-1-eda--internal-peer-review\">Checkpoint 1: EDA + Internal Peer Review (in process)</h2> <p>The checkpoint is intended to keep you on track to meet your project goals. You will need to submit an exploratory data analysis report to Gradescope. This will include submitting both a report of your results so far as well as all code necessary to replicate your results. Please answer all the questions below. Your submission should include:</p> <!-- - **Project Introduction and Goals:** Please briefly introduce your project. Think about introducing your project to someone who has a background in data science but does not know the dataset and your research question. This part should not exceed 500 words. Here are some components to help you get started: - What is the dataset about? How was the data collected? What are the available features and information? What is the size of the dataset? - What questions do you plan to ask about the dataset? Why do we care about such a problem? - What is your workflow for the project? Your first step, second step… - What are the models you plan to use? Why would the model be a good fit for your project? What are potential pitfalls you could run into? - What is your goal for the project? What are the expected deliverables? --> <ul> <li><strong>Data Sampling and Collection</strong> <ul> <li>How was the data read and sampled for your EDA process?</li> <li>Was there any potential bias introduced in the sampling process?</li> </ul> </li> <li><strong>Data Cleaning</strong> <ul> <li>What type of data are you currently exploring?</li> <li>What is the granularity of the data?</li> <li>What does the distribution of the data look like? Are there any outliers? Are there any missing or invalid entries?</li> <li>The data is not structured. How did you turn it into a structured format? What features have you engineered?</li> </ul> </li> <li><strong>Exploratory Data Analysis</strong> <ul> <li>Is there any correlation between the variables you are interested in exploring?</li> <li>How would you cleanly and accurately visualize the relationship among variables?</li> <li>What are your EDA questions? (For example, are there any relationships between A and B? What is the distribution of A?).</li> <li>Do you need to perform data transformations?</li> </ul> </li> <li><strong>Figures(tables, plots, etc.)</strong> <ul> <li>Descriptions of your figures. Takeaways from the figures.</li> <li>These figures must be of good quality (i.e. they must include axes, titles, labels, etc.) and they must be relevant to your proposed analysis.</li> </ul> </li> </ul> <p>Concretely, here are the minimal requirements for EDA for each project. Using your knowledge from Data 200, what would be appropriate data visualizations? You are welcome to do more than the minimal requirements.</p> <ul> <li><strong>Computer Vision</strong>: <ul> <li>Number of images per disaster.</li> <li>Image sizes in each dataset. Should ideally observe large variance in sizes, but similar distribution for each disaster.</li> <li>Damage labels. Should observe imbalances in the labels.</li> <li>(Open-ended) Visualize the distribution of color for different disasters.</li> <li>(Open-ended) Convey that the distributions are “separable” somehow.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"rubrics\">Rubrics</h2> <p>This section includes a rubric for how different project deliverables are going to be graded. This section will be updated as we get further along the project timeline.</p> <h3 id=\"group-formation--research-proposal-5\">Group formation + Research Proposal (5%)</h3> <ul> <li>Short paragraph description of implementation plan and timeline (2%).</li> <li>Forming teams by the deadline (3%).</li> </ul> <h3 id=\"checkpoint-1-eda--internal-peer-review-10\">Checkpoint 1: EDA + Internal Peer Review (10%)</h3> <ul> <li>Data Sampling and Collection (0.5%).</li> <li>Data Cleaning (3%).</li> <li>Exploratory Data Analysis (3%).</li> <li>Figures (tables, plots, etc.) (3%).</li> <li>Internal Peer Review (0.5%).</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
